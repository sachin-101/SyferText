{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Tri-gram Biomedical Language Model on Multiple Private Datasets\n",
    "\n",
    "\n",
    "\n",
    "Let `Bob` and `Alice` be the owner's of two hospitals. Each of there hospitals have alot of data, which can be used to train deep learning models to help doctors and medical staff to save more lives. But the hospitals can't share their data with outside parties, as it violates the privacy of the patients and is illegal in there in countries.\n",
    "They have approached you, a leading company in BioMedical Deep Learning, to find a solution to their problem. Now you are experienced in using **PyTorch**, which is an **amazing Deep Learning framework** and is great for training DL model locally. But since you are dealing with datasets which you can't see and are on another machine, you need something more than just PyTroch.\n",
    "\n",
    "- You find about **PySyft**, which is again an amazing library which overloads PyTorch with tons of features, which include sending tensors and models across machines, and perform computation on them remotely.\n",
    "    \n",
    "    \n",
    "    - But since you are dealing with text data, and can't directly feed text into models, you need something more.\n",
    "\n",
    "\n",
    "- Then you find about **SyferText**, which leverages PySyft and allows you to define text processing pipelines, which enable you to perform Natural Language Processing on remote private datasets while preserving it's privacy.\n",
    "    \n",
    "    \n",
    "    - And ... That's it! Now, you have all the tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading this tutorial, if you are unware about the theory behind `Word Embeddings` and `N-gram language models`, then I suggest you to go through this awesome, PyTorch tutorial : https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "Awesome tutorials, are one of the main reason behind PyTorch's success as a DL framework. And we have tried our best to follow the same here.\n",
    "\n",
    "You can also refer to the following tutorials, for knowing more about `Federated Learning`.\n",
    "\n",
    "- 1. [Intro to federated learning](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2002%20-%20Intro%20to%20Federated%20Learning.ipynb)\n",
    "- 2. [Federated learning via Trusted Aggregator](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2004%20-%20Federated%20Learning%20via%20Trusted%20Aggregator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/sachin/miniconda3/envs/openmined/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# SyferText imports\n",
    "import syfertext\n",
    "from syfertext.pipeline import SimpleTagger\n",
    "from syfertext.workers.virtual import VirtualWorker\n",
    "from syfertext.encdec import shared_prime, shared_base\n",
    "\n",
    "# Import useful utility functions for this tutoria\n",
    "from utils import download_dataset\n",
    "\n",
    "# PySyft imports\n",
    "import syft as sy\n",
    "from syft.generic.string import String\n",
    "from syft.generic.pointers.string_pointer import StringPointer\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Useful imports\n",
    "import os\n",
    "import tqdm\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the dataset\n",
    "\n",
    "We will train our Language Model on the [Medical Transcriptions dataset available on Kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions).\n",
    "\n",
    "Thanks to [socd06](https://github.com/socd06/medical-nlp) for processing the data and extracting the stop words and vocab from it. Althogh we will not be using the vocab.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset's root folder\n",
    "root_path = os.path.join('.', 'medical-nlp')\n",
    "\n",
    "# The URL template to all dataset files\n",
    "if not os.path.exists(root_path):\n",
    "    !git clone https://github.com/socd06/medical-nlp/\n",
    "\n",
    "data_path = os.path.join(root_path, 'data')\n",
    "assert os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprare work environment\n",
    "\n",
    "We will simulate a work environment where we will have the three hospitals, and the awesome data science expert me.\n",
    "\n",
    "We will also have the secure_worker which will act as a **neutral trusted third-party** and will help us to generate a combined vocabulary for all the hospitals and also act as a gradient aggregator when training in a federated fashion.\n",
    "\n",
    "In practice it can be a regular **AWS Instance**, connected to the private grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Create a torch hook for PySyft\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "me = sy.local_worker\n",
    "bob = VirtualWorker(hook, id='bob_hospital')\n",
    "alice = VirtualWorker(hook, id='alice_hospital')\n",
    "\n",
    "secure_worker = VirtualWorker(hook, id='secure_worker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Private Datasets\n",
    "\n",
    "Now we will simulate two private datasets owned by two different hospitals, Bob_hospital and Alice_hospital.\n",
    "\n",
    "1. Load the data from mtsamples.csv locally.\n",
    "2. Split the dataset into training and validation datasets for each worker, thus creating four splits namely, `bob_train_data`, `alice_train`, `bob_val` and `alice_val`.\n",
    "3. Each element in the four datasets, will be sent to the appropriate worker. And then we will hold pointers to the data on remote machines.\n",
    "\n",
    "`NOTE:  In a real-world scenario, the datasets will be already present on the remote machines, and will be tagged by the grid, which makes them easy to search and load.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset 4966\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       transcription\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...\n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...\n",
       "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...\n",
       "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...\n",
       "4  1.  The left ventricular cavity size and wall ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the public dataset\n",
    "csv_path = os.path.join(data_path, 'mtsamples.csv')\n",
    "dataset = pd.read_csv(csv_path, usecols=['transcription'], index_col=False).dropna()\n",
    "\n",
    "print(\"Size of dataset\", len(dataset))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subjective  this 23yearold white female presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>past medical history he has difficulty climbin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>history of present illness  i have seen abc to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2d mmode  1  left atrial enlargement with left...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1  the left ventricular cavity size and wall t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       transcription\n",
       "0  subjective  this 23yearold white female presen...\n",
       "1  past medical history he has difficulty climbin...\n",
       "2  history of present illness  i have seen abc to...\n",
       "3  2d mmode  1  left atrial enlargement with left...\n",
       "4  1  the left ventricular cavity size and wall t..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Do it using SimpleTaggers ?\n",
    "def preprocess(text):\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #remove strings like \"1.\" or \"2.\"\n",
    "    text = re.sub(r'\\d+\\.', '', text)\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove whitespaces from beginning and end\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "dataset['transcription'] = dataset['transcription'].apply(lambda x: preprocess(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size' of Bob's dataset    : Train:  2234 \tValidation:  249\n",
      "Size' of Alice's dataset  : Train:  2234 \tValidation:  249\n"
     ]
    }
   ],
   "source": [
    "# Distribute the dataset to Bob and Alice\n",
    "bob_dataset, alice_dataset = train_test_split(dataset, train_size = 0.5)\n",
    "\n",
    "bob_train,   bob_val   = train_test_split(bob_dataset,   train_size=0.9)\n",
    "alice_train, alice_val = train_test_split(alice_dataset, train_size=0.9)\n",
    "\n",
    "print(\"Size' of Bob's dataset    : Train: \", len(bob_train),  \"\\tValidation: \", len(bob_val))\n",
    "print(\"Size' of Alice's dataset  : Train: \", len(alice_train), \"\\tValidation: \", len(alice_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combining their datasets we get a much bigger dataset to train upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combined datasets : Train: 4468 \tValidation: 498\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of combined datasets : Train: {} \\tValidation: {}\".format(\n",
    "                        len(bob_train) + len(alice_train),\n",
    "                        len(bob_val) + len(alice_val))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the datasets remote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_remote(dataset, worker):\n",
    "    \"\"\"A handy function to send the dataset to a remote worker.\n",
    "    Returns a list of dictionries contaring pointers to the dataset\n",
    "    examples on the remote worker.\n",
    "    \"\"\"\n",
    "    remote_dataset = list()\n",
    "    \n",
    "    for index, example in dataset.iterrows():\n",
    "        \n",
    "        # Get the value for the current example\n",
    "        trans = example['transcription']\n",
    "        \n",
    "        # Send it to the remote worker\n",
    "        trans_pointer = String(trans).send(worker)\n",
    "        \n",
    "        # Store the pointer in dict\n",
    "        remote_dataset.append({'transcription': trans_pointer})\n",
    "    \n",
    "    return remote_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob's remote datasets\n",
    "bob_train = make_dataset_remote(bob_train, bob)\n",
    "bob_val = make_dataset_remote(bob_val, bob)\n",
    "\n",
    "# Alice's remote dataets\n",
    "alice_train = make_dataset_remote(alice_train, alice)\n",
    "alice_val = make_dataset_remote(alice_val, alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how an element of Bob's dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trans type: <class 'syft.generic.pointers.string_pointer.StringPointer'>\n",
      "Trans location: <VirtualWorker id:bob_hospital #objects:2483>\n"
     ]
    }
   ],
   "source": [
    "example = bob_train[10]\n",
    "\n",
    "trans = example['transcription']\n",
    "print(\"Trans type:\", type(trans))\n",
    "print(\"Trans location:\", trans.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it is of type `StringPointer`, pointing to real `String` object on Bob's machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pipeline\n",
    "\n",
    "To process the remote strings, we will initialize a `SyferText` Language object. At intialization the Language object contains a `Pipeline` with the `Tokenizer`. And we can add other components to the pipepline, example a `SimpleTagger` which can tag `Tokens`.\n",
    "\n",
    "Refer to [this tutorial]() for detailed understanding of `SimpleTagger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = syfertext.load('en_core_web_lg', owner=me)\n",
    "\n",
    "# Assert tokenizer is present in pipeline\n",
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a tagger for tagging stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the clinical stop words file\n",
    "with open(os.path.join(data_path, 'clinical-stopwords.txt'), 'r') as f:\n",
    "    stop_words = [line for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'},\n",
       " {'remote': True, 'name': 'stopwords_tagger'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_tagger = SimpleTagger(attribute='is_stop', \n",
    "                                lookups=stop_words, \n",
    "                                tag=True)\n",
    "\n",
    "# Add tagger with remote = True\n",
    "# This allows the tagger to be sent to remote machines\n",
    "nlp.add_pipe(stopwords_tagger, name='stopwords_tagger', remote=True)\n",
    "\n",
    "# Check pipeline\n",
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The `StopWord Tagger` will allow us to tag stop word tokens. This will help us to avoid them, while training our Word Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process remote Strings using `SyferText` Pipeline\n",
    "\n",
    "Now that we have our Pipeline ready, we can feed our remote Strings into the pipeline. Based on the components in Pipeline, the following processing steps take place:\n",
    "\n",
    "1. `Tokenizer` breaks the strings into multiple tokens.\n",
    "2. `Stop-word Tagger` tags the stop-word tokens based on the `lookup` that we have provided to it.\n",
    "\n",
    "After each string is processed by the pipeline, we are returned a pointer to the remote `Doc` object. This remote Doc holds all the tokens, and exposes tools via it's pointer which allow us call certain methods on the Doc which don't violate the privacy of it's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, dataset_name, print_interval):\n",
    "        \n",
    "    for i, example in enumerate(dataset):\n",
    "        \n",
    "        # Process the current example\n",
    "        example['transcription'] = nlp(example['transcription'])\n",
    "        \n",
    "        if (i+1)%print_interval == 0:\n",
    "            print(dataset_name, '\\t', f'{i+1}/{len(dataset)}', end='\\r', flush=True)\n",
    "    \n",
    "    print(dataset_name, \"Processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob_train Processed34\n"
     ]
    }
   ],
   "source": [
    "# Process bob_train dataset\n",
    "# NOTE: THIS WILL TAKE TIME\n",
    "process_dataset(bob_train, \"bob_train\", print_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_train Processed34\n"
     ]
    }
   ],
   "source": [
    "# Process alice_train dataset\n",
    "# NOTE: THIS WILL TAKE TIME\n",
    "process_dataset(alice_train, \"alice_train\", print_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed our data, and tagged our tokens, we can go ahead and create our vocabulary by combining the vocabularies across bob's dataset and alice's dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Combined Vocabulary\n",
    "\n",
    "Now we have to create a `Combined Vocabulary` by combining the vocabularies of Bob and Alice, and assign each unique word in the combined vocabulary an unique index. \n",
    "\n",
    "But building a combined vocabulary requires us to know the set of words that are present in each dataset. But this info may reveal certain aspects of the data, for example, in our case, it may reveal the names of medicines & drugs used by each hospital, and machines that they have and other information. Which violates the privacy of involved hospitals.\n",
    "\n",
    "To solve this problem we have a `Secure Worker` which is a trusted third party server, which performs the `Private Set Intersection` of each worker's vocabulary for us. It also assigns each word in the combined vocabulary an unique index and returns the vocabulary mapped to indices back to the workers.\n",
    "As mentioned before, `Secure Worker` can be a cloud server, such as an **AWS Instance**.\n",
    "\n",
    "For more details, refer to this tutorial:\n",
    "\n",
    "And thanks to this Stack Exchange answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a Diffie-Hellman key exchange to encrypt the\n",
    "# vocabularies of each worker\n",
    "workers = [bob, alice]\n",
    "secure_worker.execute_dh_key_exchange(shared_prime, shared_base, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can gather together the vocabulary from each worker in a secure manner, and combine them to create a unified vocabulary.\n",
    "\n",
    "We can also **exclude certain tokens, from being a part of the combined vocabulary**, if we are not going to use them for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 43912\n"
     ]
    }
   ],
   "source": [
    "# Exclude stop words from the combined vocabulary\n",
    "excluded_tokens = {\"is_stop\": {True}}\n",
    "\n",
    "# NOTE: THIS WILL ALSO TAKE SOME TIME\n",
    "\n",
    "vocab_size = secure_worker.create_vocabulary(bob_train + alice_train, 'transcription', excluded_tokens)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Federate Training?\n",
    "\n",
    "Now that we have assigned unique indices to each word in the combined vocabulary, we can bring the contents of the remote documents in form of their conrresponding index and since we don't know the mapping from the indices to the words, we will not be able to extract the contents of the documents.\n",
    "\n",
    "This above idea may seem right to you.. but actually it is not. :)\n",
    "\n",
    "`There are multiple ways to violate PRIVACY, there is only one way to maintain it.`\n",
    "\n",
    "Evne when we bring back words mapped to their indices, we can perform a frequency attack to establish a relation between the indices and the correspondig words that they represent. To avoid this, we will train our model in a `Federated` fahsion. \n",
    "\n",
    "\n",
    "\n",
    "In federated training, instead of collecting all the data to a single place, for the model to train on. We instead, send the model to the machine, where the data resides. This maintains the `ownership` of the data, with the data provider and also maintains it's privacy. If you have gone through the tutorials mentioned above, you would know that, bringing the gradients, directly from a worker may leak information about the worker.\n",
    "\n",
    "So, in order to avoid it, we perform `gradient averaging on a Secure Aggregator`. Secure Aggregator, basically aggregates the gradients from all workers and averages them before returning to us. This disables us from learning\n",
    "a lot about a single data provider, thus preserving their individual privacy.\n",
    "\n",
    "There are other much more secure way of averaging gradients such as `Encrypted Gradient Aggregation`, but they aren't supported in PySyft yet. Here is the [issue link](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb) if you want to contribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can use our `Secure Worker` which has helped us to make the `combined vocabulary` as the `Secure Aggregator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `combined vocabulary` with unique indices assigned to each word. Let's create a Dataset class, which will return us pointers to `Context` and `Target` vectors for a given remote `Document`. \n",
    "These Context and target pairs can be used to train our `N-gram language model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    \"\"\"Creates an n-gram dataset combining remote datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, n_gram, excluded_tokens, nlp = None):\n",
    "        \"\"\"Initialize the Dataset object\n",
    "        \n",
    "        Args:\n",
    "            dataset (list): List containing examples as dicts.\n",
    "            n_gram (int): Size of n-grams to be created for training.\n",
    "            excluded_tokens (Dict): Tokens to be excluded while making\n",
    "                context and target pairs.\n",
    "            nlp (Language): The syfertext language object containing\n",
    "                the pipeline. Should be passed in when you pass a dataset\n",
    "                which has not been already processed by the pipeline.\n",
    "                Example, during validation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.n_gram = n_gram\n",
    "        self.excluded_tokens = excluded_tokens\n",
    "        self.nlp = nlp\n",
    "        \n",
    "        self._create_relative_context_pos()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"In this function, we will be returned a pointer to the \n",
    "        context, target pairs from the document.\n",
    "        \n",
    "        Args:\n",
    "            index (int): index of the example to be fetched.\n",
    "                This actually indexes one example in `self.dataset`\n",
    "                which pools over examples of all the remote datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the example\n",
    "        example = self.dataset[index]\n",
    "        doc_ptr = example['transcription']\n",
    "        \n",
    "        if isinstance(doc_ptr, StringPointer):\n",
    "            doc_ptr = self.nlp(doc_ptr)\n",
    "        \n",
    "        # Get a pointer to the context and target tensors\n",
    "        # of dtype = torch.LongTensor residing on the worker\n",
    "        # to whom this example belongs.\n",
    "        \n",
    "        context, target = doc_ptr.get_context_target_tensors(self.relative_context_pos,\n",
    "                                                             self.excluded_tokens)\n",
    "        \n",
    "        return context, target\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the combined size of all of the \n",
    "        remote training/validation sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The size of the combined datasets\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def _create_relative_context_pos(self):\n",
    "        \"\"\"The doc.get_context_target_tensors()` requires us to pass\n",
    "        the relative position of the context tokens with respect to the \n",
    "        target tokens. \n",
    "        \n",
    "        Example: If we are training a tri-gram language model, \n",
    "            then for given target token position `i`. The context positions\n",
    "            will be [i-2, i-1]. So the corresponding relative context\n",
    "            position will be [-2, -1].\n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        context_size = self.n_gram - 1\n",
    "        self.relative_context_pos = [-(context_size - i) \n",
    "                                     for i in range(context_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create two such NGramDataset objects, one for training and the other for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also instantiate an n-gram variable\n",
    "N_GRAM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude stop words from being a part of \n",
    "# context, or from being a target\n",
    "excluded_tokens = {\"is_stop\": {True}}\n",
    "\n",
    "\n",
    "trainset = NGramDataset(dataset = list(bob_train + alice_train),\n",
    "                      n_gram = N_GRAM, \n",
    "                      excluded_tokens = excluded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model Class\n",
    "\n",
    "We will create a Model with the following layers.\n",
    "1. Embedding Layer : \n",
    "    - Input: A torch.LongTensor representing the indices which form the context, eg. [2, 3, 4]\n",
    "    - Output: Embedding vectors of the input indices, which are stacke toghether and fed to next layer.\n",
    "2. Linear layer: Takes embedding vectors as the input and outputs a 128 dimensional vector \n",
    "3. Linear layer: Takes 128 dimensional vector and maps outputs a vector with dimension of vocabulary size. And then apply LogSoftamx activation on the output of the last layer.\n",
    "\n",
    "Finally since we have used LogSoftmax activation in our final layer, we use nn.NLLLoss as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of combined vocabulary\n",
    "            embedding_dim: Embedding dimension\n",
    "            context_size: (n_gram - 1) is context size\n",
    "        \"\"\"\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: A torch.LongTensor having indices for the context\n",
    "                dimensions = (batch_size, context_size)\n",
    "        Returns:\n",
    "            log_probs: dimension = (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = F.relu(self.linear1(embeds.view(embeds.shape[0], -1)))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our model, which will be later copied and send to Bob's and Alice's machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = NGramLanguageModeler(vocab_size, \n",
    "                             embedding_dim = EMBEDDING_DIM, \n",
    "                             context_size = N_GRAM - 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our model !! \n",
    "\n",
    "Finally it's time to train our model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, loss_function, dataset):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Reset total loss\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Send updated copy of model to bob and alice\n",
    "        bobs_model = model.copy().send(bob)\n",
    "        alices_model = model.copy().send(alice)\n",
    "        \n",
    "        # Initilaize optimizers for the remote models\n",
    "        optimizers = {'bob_hospital'  : optim.SGD(bobs_model.parameters(), lr=0.01),\n",
    "                      'alice_hospital': optim.SGD(alices_model.parameters(), lr=0.01)}\n",
    "        \n",
    "        # Map models to their workers\n",
    "        models = {'bob_hospital': bobs_model, 'alice_hospital': alices_model}\n",
    "               \n",
    "        for contexts, targets in dataset:\n",
    "            \n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            \n",
    "            if(contexts.shape[0] > 0):\n",
    "                # get the location where this data resides\n",
    "                location = contexts.location.id\n",
    "\n",
    "                # Zero out the gradients\n",
    "                optimizers[location].zero_grad()\n",
    "\n",
    "                # Perform a prediction            \n",
    "                probs = models[location](contexts)\n",
    "\n",
    "                # Calculate by how much we missed\n",
    "                loss = loss_function(probs, targets)\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizers[location].step()\n",
    "\n",
    "                # update total loss\n",
    "                total_loss += loss.get().item()\n",
    "\n",
    "        # Move bob's and alice's models to Secure worker\n",
    "        # acting as the seucre aggregator here\n",
    "        bobs_model.move(secure_worker)\n",
    "        alices_model.move(secure_worker)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Average the parmams(weights and biases) on secure aggregator\n",
    "            # And update our local model with the new params\n",
    "            updated_state_dict = {}\n",
    "            \n",
    "            for param in model.state_dict().keys():\n",
    "                updated_state_dict[param] = ( (bobs_model.state_dict()[param] + \n",
    "                                               alices_model.state_dict()[param]) / 2\n",
    "                                            ).get()\n",
    "                \n",
    "            model.load_state_dict(updated_state_dict)\n",
    "            \n",
    "        # Print loss\n",
    "        total_loss /= len(dataset)\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss: 9.14086214185508\n",
      "Epoch:  1 \t Loss: 8.798042418386856\n",
      "Epoch:  2 \t Loss: 8.503424681823171\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 3.82 GiB total capacity; 2.32 GiB already allocated; 203.69 MiB free; 2.89 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-71b6609b567d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-6ff6b9fef880>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, loss_function, dataset)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/trace.py\u001b[0m in \u001b[0;36mtrace_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP/SyferText/syfertext/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"\"\"receive message\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_tensor_command\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_tensor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPointerTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_computation_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_communication_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_computation_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to properly deserialize strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/trace.py\u001b[0m in \u001b[0;36mtrace_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/syft-0.2.4-py3.6.egg/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openmined/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 3.82 GiB total capacity; 2.32 GiB already allocated; 203.69 MiB free; 2.89 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "\n",
    "train(epochs, loss_function, trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataset class for bob's and alice's validation datasets\n",
    "\n",
    "bob_valset = NGramDataset(dataset = bob_val,\n",
    "                          n_gram = N_GRAM, \n",
    "                          excluded_tokens = excluded_tokens,\n",
    "                          nlp = nlp)\n",
    "\n",
    "alice_valset = NGramDataset(dataset = alice_val,\n",
    "                            n_gram = N_GRAM, \n",
    "                            excluded_tokens = excluded_tokens,\n",
    "                            nlp = nlp)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModeler(\n",
       "  (embeddings): Embedding(43912, 100)\n",
       "  (linear1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=43912, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move stuff temporarily to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(worker, val_dataset):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Send copy of model to remote worker\n",
    "    remote_model = model.copy().send(worker)\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    for i in range(len(val_dataset)):\n",
    "        \n",
    "        try:\n",
    "            contexts, targets = val_dataset[i]\n",
    "            \n",
    "            contexts, targets = contexts.to(device), targets.to(device)\n",
    "            \n",
    "            if(contexts.shape[0] > 0):\n",
    "                \n",
    "                \n",
    "                # Perform a prediction\n",
    "                probs = remote_model(contexts)\n",
    "\n",
    "                # Get the predicted labels\n",
    "                preds = probs.argmax(dim=1)\n",
    "\n",
    "                # Compute the prediction accuracy\n",
    "                accuracy = (preds == targets).sum()\n",
    "                accuracy = accuracy.get().item()\n",
    "                accuracy = 100 * (accuracy / contexts.shape[0])\n",
    "\n",
    "                # Add to total accuracy\n",
    "                total_accuracy += accuracy\n",
    "                \n",
    "        \n",
    "        except KeyError: # OOV token\n",
    "            pass\n",
    "    \n",
    "    total_accuracy = total_accuracy/len(val_dataset)\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.606402360514078"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on bob's validation dataset\n",
    "test(bob, bob_valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.722117200860472"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on alice's validation dataset\n",
    "test(alice, alice_valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "0. Think about how to handle batch of context, target pairs.\n",
    "0. Add OOV to combined_vocab.\n",
    "1. ~Move to GPU.~\n",
    "2. Make the vocabulary creation more secure.\n",
    "3. Make the vocab set serializable and sendable.\n",
    "4. How to avoid .get() on PointerTensors ?\n",
    "5. Add tensorboard ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Owenership of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('openmined': conda)",
   "language": "python",
   "name": "python361064bitopenminedconda7d626f6923e74ad6af0109078ac21f5f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
